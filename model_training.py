# -*- coding: utf-8 -*-
"""FINAL_MODEL_TRAINING.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/193MK7FQmT7hi2xeFimjjXQb6OetbD3Ae
"""

from google.colab import drive
drive.mount('/content/drive')

import os
notebook_path = os.path.abspath(".")
os.chdir('/content/drive/MyDrive/DLIP_FINAL')
print("현재 노트북 디렉토리 경로:", notebook_path)

!pip install piq

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torchvision import models
from torchsummary import summary
from torch.utils.data import DataLoader
import piq

# Get cpu or gpu device for training.
device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Using {device} device")

if torch.cuda.is_available(): print(f'Device name: {torch.cuda.get_device_name(0)}')

transform_train = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.RandomRotation(10),
    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ToTensor(),
    #transforms.Normalize(mean=[0.5], std=[0.5])
])


transform_test = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    #transforms.Normalize(mean=[0.5], std=[0.5])
])

train_dataset = torchvision.datasets.ImageFolder(root='./final_dataset/train', transform=transform_train)
train_loader=DataLoader(train_dataset,batch_size=4,shuffle=True)

test_dataset=torchvision.datasets.ImageFolder(root='./final_dataset/test',transform=transform_test)
test_dataset.class_to_idx = {'good':0, 'anomaly':1}  # 원하는 매핑으로 재정의
test_loader=DataLoader(test_dataset,batch_size=4, shuffle=True)
test_dataset.samples = [
    (path, test_dataset.class_to_idx[os.path.basename(os.path.dirname(path))])
    for path, _ in test_dataset.samples
]

anomaly_test_dataset=torchvision.datasets.ImageFolder(root='./final_dataset/anomaly',transform=transform_test)
anomaly_test_dataset.class_to_idx = {'anomaly':1}  # 원하는 매핑으로 재정의
anomaly_test_loader=DataLoader(anomaly_test_dataset,batch_size=4, shuffle=True)
anomaly_test_dataset.samples = [
    (path, anomaly_test_dataset.class_to_idx[os.path.basename(os.path.dirname(path))])
    for path, _ in anomaly_test_dataset.samples
]

batch_size=4
c=0
for X,y in train_loader:
  print(f"Shape of X [N, C, H, W]: {X.shape} {y.dtype}")
  c+=1

  if c==1:
    break

class ConvAutoencoder(nn.Module):
    def __init__(self):
        super(ConvAutoencoder, self).__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),   # [B, 64, 256, 256]
            nn.ReLU(),
            nn.MaxPool2d(2, 2, padding=0),    # [B, 64, 128, 128]
            nn.Conv2d(64, 128, 3, padding=1), # [B, 128, 128, 128]
            nn.ReLU(),
            nn.MaxPool2d(2, 2, padding=0),    # [B, 128, 64, 64]
            nn.Conv2d(128, 256, 3, padding=1),# [B, 256, 64, 64]
            nn.ReLU(),
            nn.MaxPool2d(2, 2, padding=0)     # [B, 256, 32, 32]
        )
        # Decoder
        self.decoder = nn.Sequential(
            nn.Conv2d(256, 256, 3, padding=1),# [B, 256, 32, 32]
            nn.ReLU(),
            nn.Upsample(scale_factor=2),      # [B, 256, 64, 64]
            nn.Conv2d(256, 128, 3, padding=1),# [B, 128, 64, 64]
            nn.ReLU(),
            nn.Upsample(scale_factor=2),      # [B, 128, 128, 128]
            nn.Conv2d(128, 64, 3, padding=1), # [B, 64, 128, 128]
            nn.ReLU(),
            nn.Upsample(scale_factor=2),      # [B, 64, 256, 256]
            nn.Conv2d(64, 3, 3, padding=1),   # [B, 3, 256, 256]
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x


input_size=256
model=ConvAutoencoder().to(device)

summary(model,(3,input_size,input_size))

class HybridLoss(nn.Module):
  def __init__(self,alpha=0.8):
    super(HybridLoss,self).__init__()
    self.alpha=alpha
    self.mse=nn.MSELoss()

  def forward(self,recon,origin):

    ssim_loss=1-piq.ssim(recon,origin,data_range=1.0)
    mse_loss=self.mse(recon, origin)

    return self.alpha*mse_loss+(1-self.alpha)*ssim_loss


loss_fn=HybridLoss()

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

epochs=30

for epochs in range(epochs):
  model.train()
  running_loss=0.0

  for batch in train_loader:
    inputs,_=batch
    inputs=inputs.to(device)

    #forward
    outputs=model(inputs)
    loss=loss_fn(outputs,inputs)

    #back propagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    running_loss+=loss.item()

  avg_loss=running_loss/len(train_loader)
  print(f"Epoch [{epochs+1}/{epochs}] Loss: {avg_loss:.5f}")

import matplotlib.pyplot as plt
import torchvision.utils as vutils
import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

model.eval()
with torch.no_grad():
    for inputs, _ in train_loader:
        inputs = inputs.to(device)
        outputs = model(inputs)

        # 첫 번째 배치만 보기
        break



# 시각화
n = 4  # 보여줄 이미지 수
plt.figure(figsize=(12, 4))
for i in range(n):
    # 원본
    plt.subplot(2, n, i+1)
    plt.imshow(inputs[i].permute(1, 2, 0).cpu().numpy())
    plt.title("Original")
    plt.axis("off")

    # 재구성 결과
    plt.subplot(2, n, i+1+n)
    plt.imshow(outputs[i].permute(1, 2, 0).cpu().numpy())
    plt.title("Reconstructed")
    plt.axis("off")

plt.show()

def evaluate_accuracy(model, dataloader, loss_fn, threshold):

    model.eval()
    all_errors = []
    all_labels = []
    correct = 0
    total = len(dataloader.dataset)

    total_mse = 0.0
    total_ssim = 0.0
    batch_count = 0

    with torch.no_grad():

      for inputs, labels in dataloader:
          inputs = inputs.to(device)
          labels = labels.to(device)
          outputs = model(inputs)

          # 재구성 에러 (MSE)
          reconstruction_error = F.mse_loss(outputs, inputs, reduction='none')
          reconstruction_error = reconstruction_error.mean(dim=[1,2,3])  # batch 단위 평균
          all_errors.extend(reconstruction_error.cpu().numpy())
          all_labels.extend(labels.cpu().numpy())



          # 예측: 오류가 threshold보다 크면 anomaly (1), 작으면 normal (0)
          preds = (reconstruction_error > threshold).float()

          # normal=0, anomaly=1 라벨로 구성되어 있음
          correct += (preds == labels).sum().item()

          # 전체 평균 MSE
          batch_mse = F.mse_loss(outputs, inputs, reduction='mean').item()
          total_mse += batch_mse

          # 전체 평균 SSIM
          ssim_value = piq.ssim(outputs, inputs, data_range=1.0).item()
          total_ssim += ssim_value

          batch_count += 1



    acc = correct / total
    avg_mse = total_mse / batch_count
    avg_ssim = total_ssim / batch_count
    ssim_loss = 1 - avg_ssim
    print(f'✅ Test Accuracy: {acc * 100:.2f}% (Threshold: {threshold})\n')
    print(f'Average MSE: {avg_mse:.6f}')
    print(f'Average SSIM: {avg_ssim:.6f} (SSIM Loss: {ssim_loss:.6f})\n')

    return acc, all_errors, all_labels

print("Testing for All test dataset")

acc, all_errors, all_labels = evaluate_accuracy(model, test_loader, loss_fn, threshold=0.1)

plt.figure(figsize=(8, 5))
plt.hist(all_errors, bins=30, color='mediumslateblue', edgecolor='white')
plt.title("Reconstruction Error Distribution (test)")
plt.xlabel("Reconstruction Error")
plt.ylabel("Count")
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

print("Testing only good dataset")
acc, all_errors, all_labels  = evaluate_accuracy(model, train_loader, loss_fn, threshold=0.1)

plt.figure(figsize=(8, 5))
plt.hist(all_errors, bins=30, color='mediumslateblue', edgecolor='white')
plt.title("Reconstruction Error Distribution (good)")
plt.xlabel("Reconstruction Error")
plt.ylabel("Count")
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

print("Testing only anomaly dataset")
acc, all_errors, all_labels = evaluate_accuracy(model, anomaly_test_loader, loss_fn, threshold=0.00066)

plt.figure(figsize=(8, 5))
plt.hist(all_errors, bins=30, color='mediumslateblue', edgecolor='white')
plt.title("Reconstruction Error Distribution (scratch)")
plt.xlabel("Reconstruction Error")
plt.ylabel("Count")
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

def plot_recon_error_histogram_from_evaluation(errors, labels):
    normal_errors = [e for e, l in zip(errors, labels) if l == 0]
    anomaly_errors = [e for e, l in zip(errors, labels) if l == 1]

    plt.figure(figsize=(8, 5))
    plt.hist(normal_errors, bins=50, alpha=0.6, label='Normal', color='blue')
    plt.hist(anomaly_errors, bins=50, alpha=0.6, label='Anomaly', color='red')
    plt.axvline(np.mean(normal_errors), color='blue', linestyle='dashed', label='Normal Mean')
    plt.axvline(np.mean(anomaly_errors), color='red', linestyle='dashed', label='Anomaly Mean')
    plt.title("Reconstruction Error Histogram")
    plt.xlabel("Reconstruction Error")
    plt.ylabel("Count")
    plt.legend()
    plt.grid(True)
    plt.show()

acc, errors, labels = evaluate_accuracy(model, test_loader, loss_fn, threshold=0.0012)
plot_recon_error_histogram_from_evaluation(errors, labels)

import matplotlib.pyplot as plt
import torch.nn.functional as F

def show_error_map(input_tensor, output_tensor, index=0, cmap='hot'):
    """
    input_tensor, output_tensor: shape [B, C, H, W] (normalized [0,1])
    index: 배치 중 시각화할 이미지 인덱스
    """
    # 1. MSE error map 계산
    with torch.no_grad():
        error_map = F.mse_loss(output_tensor, input_tensor, reduction='none')  # [B, C, H, W]
        error_map = error_map.mean(dim=1)  # [B, H, W]

    # 2. numpy로 변환
    input_img = input_tensor[index].permute(1, 2, 0).cpu().numpy()
    output_img = output_tensor[index].permute(1, 2, 0).cpu().numpy()
    error_img = error_map[index].cpu().numpy()

    # 3. 시각화
    fig, axs = plt.subplots(1, 3, figsize=(15, 5))
    axs[0].imshow(input_img)
    axs[0].set_title("Input Image")
    axs[0].axis('off')

    axs[1].imshow(output_img)
    axs[1].set_title("Reconstructed Image")
    axs[1].axis('off')

    axs[2].imshow(error_img, cmap=cmap)
    axs[2].set_title("Reconstruction Error Map")
    axs[2].axis('off')

    plt.tight_layout()
    plt.show()

torch.save(model.state_dict(), 'model.pth')

from google.colab import files
files.download('model.pth')